{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM,AutoTokenizer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we are at device cuda\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a89f84123bb420887df30c26177c41e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/877 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05d552e3383344049d9d0a37224b6156",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.47G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49ebb3adbde540c7956537081e81d26a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/189 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "936c5744d7924f29aa463a23d233ba38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/54.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aafb7035141a40f78e2e881567114f2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe00bdee663045a1ba9e7771256a3181",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/296 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'we are at device {str(DEVICE)}')\n",
    "model_name = 'meta-llama/Llama-3.2-1B-Instruct'\n",
    "cache_dir = \"../model/llama-3.2-1B-Instruct\"\n",
    "\n",
    "# download and load model and tokenizer to specified folder\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name,cache_dir=cache_dir)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name,cache_dir=cache_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"system\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 28 Dec 2024\\n\\nYou are a helpful assistant systemuser\\n\\nhello, please introduce yourself.assistant\\n\\nHello! I'm an AI assistant, here to provide information and answer your questions to the best of my abilities. I'm a large language model, trained on a vast amount of text data, which allows me to understand and respond to a wide range of topics and questions. I'm constantly learning and improving, so please bear with me if I make any mistakes.\\n\\nI'm a friendly and neutral AI, here to assist and communicate with you in a helpful and informative way. I'm available 24/7 to chat, answer questions, provide explanations, and engage in conversation. Whether you have a specific question, topic in mind, or just want to talk, I'm here to help.\\n\\nI'm a machine learning model, so I don't have personal opinions or emotions, but I'm designed to provide accurate and unbiased information based on my training data. I can also help with language-related tasks, such as:\\n\\n* Answering questions on various topics\\n* Providing definitions and explanations\\n* Offering suggestions and ideas\\n* Translating text from one language to another\\n* Summarizing long pieces of text\\n* And much more!\\n\\nSo, what's on your mind? How can I help you today?\"]\n"
     ]
    }
   ],
   "source": [
    "prompt =\"hello, please introduce yourself.\"\n",
    "message = [{\"role\":\"system\",\"content\":\"You are a helpful assistant system\"},{\"role\":\"user\",\"content\":prompt}]\n",
    "text = tokenizer.apply_chat_template(message,tokenize=False,add_generation_prompt=True)\n",
    "model_inputs = tokenizer([text],return_tensors=\"pt\").to(DEVICE)\n",
    "model.to(DEVICE)\n",
    "generate_ids = model.generate(model_inputs.input_ids,max_new_tokens=512)\n",
    "response = tokenizer.batch_decode(generate_ids,skip_special_tokens=True)\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
